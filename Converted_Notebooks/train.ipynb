{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import time\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, config):\n",
    "\tsince = time.time()\n",
    "\n",
    "\t# validation for every 5 epoches\n",
    "\ttest_epoch = 5\n",
    "\tfor epoch in range(config.epochs):\n",
    "\t\ttrain_dev = []\n",
    "\t\tfor phase in ['train']:\n",
    "\t\t\tmodel.train(True)  # Set model to training mode\n",
    "\t\t\trunning_loss = 0.0\n",
    "\t\t\t# Iterate over data.\n",
    "\t\t\tlent = len(dataloaders[phase])\n",
    "\t\t\tpbar = tqdm(total=lent * config.batchSize)\n",
    "\t\t\tfor ide in range(lent):\n",
    "\t\t\t\tdata = dataloaders[phase][ide]\n",
    "\t\t\t\t\n",
    "\t\t\t\tinputs, labels = data['image'], data['landmarks']\n",
    "\t\t\t\tinputs = inputs.to(config.use_gpu)\n",
    "\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\t# forward\n",
    "\t\t\t\theatmaps = model(inputs)\n",
    "\n",
    "\t\t\t\t# loss calculation for one heatmap and two offset maps.\n",
    "\t\t\t\tloss = criterion(heatmaps[0], labels.detach().cpu())\n",
    "\t\t\t\t#~ # backward + optimize only if in training phase\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t\tif epoch%test_epoch == 0:\n",
    "\t\t\t\t\t# landmark prediction. The results are normalized to (0, 1)\n",
    "\t\t\t\t\tpredicted_landmarks = utils.regression_voting(heatmaps, config.R2).cuda(config.use_gpu)\n",
    "\t\t\t\t\t# deviation calculation for all landmarks\n",
    "\t\t\t\t\tdev = utils.calculate_deviation(predicted_landmarks.detach(), labels.cuda(config.use_gpu).detach())\n",
    "\t\t\t\t\ttrain_dev.append(dev)\n",
    "\n",
    "\t\t\t\trunning_loss += loss.item()\n",
    "\t\t\t\tpbar.update(config.batchSize)\n",
    "\t\t\tpbar.close()\n",
    "\n",
    "\t\t\tepoch_loss = running_loss / lent\n",
    "\t\t\tprint('{} epoch: {} Loss: {}'.format(phase, epoch, epoch_loss))\n",
    "\n",
    "\t\t# validation\n",
    "\t\tif epoch%test_epoch == 0:\n",
    "\t\t\t# result statistics\n",
    "\t\t\ttrain_dev = torch.stack(train_dev).squeeze() * config.spacing\n",
    "\t\t\ttrain_SDR, train_SD, train_MRE = utils.get_statistical_results(train_dev, config)\n",
    "\n",
    "\t\t\t# MRE is the mean radial error, SDR is the the successful detection rate in five target radius (1mm, 2mm, 2.5mm, 3mm, 4mm)\n",
    "\t\t\tprint(\"train_MRE(SD): %f(%f), SDR([1mm, 2mm, 2.5mm, 3mm, 4mm]): \" % (torch.mean(train_MRE).detach().cpu().numpy(),\n",
    "\t\t\t\t  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttorch.mean(train_SD).detach().cpu().numpy()),\n",
    "\t\t\t\t  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttorch.mean((train_SDR), 0).detach().cpu().numpy())\n",
    "\t\t\t# validation on val dataset\n",
    "\t\t\tval(model, dataloaders, criterion, optimizer, config)\n",
    "\n",
    "\ttime_elapsed = time.time() - since\n",
    "\tprint('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "\t\ttime_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "best_MRE = 10000\n",
    "best_SDR = []\n",
    "best_SD = 0\n",
    "\n",
    "def val(model, dataloaders, criterion, optimizer, config):\n",
    "\tsince = time.time()\n",
    "\ttest_dev = []\n",
    "\n",
    "\tfor phase in ['val']:\n",
    "\t\tmodel.train(False)  # Set model to evaluate mode\n",
    "\t\trunning_loss = 0.0\n",
    "\t\t# Iterate over data.\n",
    "\t\tlent = len(dataloaders[phase])\n",
    "\t\tpbar = tqdm(total=lent * config.batchSize)\n",
    "\t\tfor ide in range(lent):\n",
    "\t\t\tdata = dataloaders[phase][ide]\n",
    "\n",
    "\t\t\tinputs, labels = data['image'], data['landmarks']\n",
    "\t\t\tinputs = inputs.to(config.use_gpu)\n",
    "\t\t\t# forward\n",
    "\t\t\theatmaps = model(inputs)\n",
    "\n",
    "\t\t\t# landmark prediction. The results are normalized to (0, 1)\n",
    "\t\t\tpredicted_landmarks = utils.regression_voting(heatmaps, config.R2).to(config.use_gpu)\n",
    "\t\t\t# deviation calculation for all predictions\n",
    "\t\t\tdev = utils.calculate_deviation(predicted_landmarks.detach(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t  labels.to(config.use_gpu).detach())\n",
    "\n",
    "\t\t\ttest_dev.append(dev)\n",
    "\t\t\tpbar.update(config.batchSize)\n",
    "\t\tpbar.close()\n",
    "\n",
    "\t# statistics\n",
    "\ttest_dev = torch.stack(test_dev).squeeze() * config.spacing\n",
    "\ttest_SDR, test_SD, test_MRE = utils.get_statistical_results(test_dev, config)\n",
    "\n",
    "\t# MRE is the mean radial error, SDR is the the successful detection rate in five target radius (1mm, 2mm, 2.5mm, 3mm, 4mm)\n",
    "\tprint(\"test_MRE(SD): %f(%f), SDR([1mm, 2mm, 2.5mm, 3mm, 4mm]):\" % (\n",
    "\ttorch.mean(test_MRE).detach().cpu().numpy(),\n",
    "\ttorch.mean(test_SD).detach().cpu().numpy()),\n",
    "\t\t  torch.mean((test_SDR), 0).detach().cpu().numpy())\n",
    "\n",
    "\tglobal best_MRE\n",
    "\tglobal best_SD\n",
    "\tglobal best_SDR\n",
    "\tif best_MRE > torch.mean(test_MRE).detach().cpu().numpy():\n",
    "\t\tbest_MRE = torch.mean(test_MRE).detach().cpu().numpy()\n",
    "\t\tbest_SD = torch.mean(test_SD).detach().cpu().numpy()\n",
    "\t\tbest_SDR = torch.mean((test_SDR), 0).detach().cpu().numpy()\n",
    "\t\t# torch.save(model, \"output/\" + str(epoch) + saveName + '.pkl')\n",
    "\n",
    "\ttime_elapsed = time.time() - since\n",
    "\tprint('testing complete in {:.0f}m {:.0f}s'.format(\n",
    "\t\ttime_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "\t# MRE is the mean radial error, SDR is the the successful detection rate in five target radius (1mm, 2mm, 2.5mm, 3mm, 4mm)\n",
    "\tprint(\"Best val MRE(SD): %f(%f), SDR([1mm, 2mm, 2.5mm, 3mm, 4mm]):\" % (best_MRE, best_SD), best_SDR)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
